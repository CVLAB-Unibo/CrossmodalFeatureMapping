<section class="hero  has-text-centered" id="paper">
    <div class="hero-body">
        <div class="container">
          <h2 align="centered" class="title">Paper Depth4ToM</h2>
            <br>
              <div style="display: flex; gap: 10px; justify-content: center; align-items: center;">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Costanzino_Learning_Depth_Estimation_for_Transparent_and_Mirror_Surfaces_ICCV_2023_paper.pdf"><h2>[Paper]</h2>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Costanzino_Learning_Depth_Estimation_ICCV_2023_supplemental.pdf"><h2>[Supplementary]</h2>
                <a href="https://github.com/CVLAB-Unibo/Depth4ToM-code#-learning-depth-estimation-for-transparent-and-mirror-surfaces-iccv-2023-"><h2>[Code]</h2></a>
                <a href="https://github.com/CVLAB-Unibo/Depth4ToM-code#file_cabinet-dataset"><h2>[Dataset]</h2></a>
                <a href="https://github.com/CVLAB-Unibo/Depth4ToM/blob/4e6cc00409a0ce4674156f6f4844f077ebd7a5fb/assets/ICCV_2023_poster.pdf"><h2>[Poster]</h2></a>
                <a href="https://www.youtube.com/watch?v=qxJxcsmC0pc"></a><h2>[Video]</h2></a>
              </div>              
            <br>
                <div class="columns">
                <div class="column is-one-fifth-desktop is-one-fifth-tablet is-one-fifth-fullhd">
                  <br>
                  <br>
                  <br>
                  <a href="https://arxiv.org/abs/2307.15052">
                  <img style="width:90%" src="assets/paper_icon.png" >
                  </a>
                </div>
                <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">

                    <br>
                    <div class="container columns is-centered">
                        <div>
                          <B><a href="https://arxiv.org/abs/2307.15052"><font size = "+2">Learning Depth Estimation for Transparent and Mirror Surfaces</font></a><br></B>
                          <B><i> Alex Costanzino*, Pierluigi Zama Ramirez*, Matteo Poggi*, Fabio Tosi, Stefano Mattoccia, Luigi Di Stefano</B></i>
                          <br>
                          <i><font size = "-1">*Equal Contribution</font></i>
                          <br>
                          <br>
                          <p>
                            Inferring the depth of transparent or mirror (ToM) surfaces represents a hard challenge for either sensors, algorithms, or deep networks. 
              
                            We propose a simple pipeline for learning to estimate depth properly for such surfaces with neural networks, without requiring any ground-truth annotation. 
                            We unveil how to obtain reliable pseudo labels by in-painting ToM objects in images and processing them with a monocular depth estimation model. 
                            These labels can be used to fine-tune existing monocular or stereo networks, to let them learn how to deal with ToM surfaces. 
                            
                            Experimental results on the Booster dataset show the dramatic improvements enabled by our remarkably simple proposal.
                          </div>
                        <div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                CITATION
                </h3>
                <div class="has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen form-group col-md-18 col-md-offset-0">
<pre>
@inproceedings{costanzino2023iccv,
    title = {Learning Depth Estimation for Transparent and Mirror Surfaces},
    author = {Costanzino, Alex and Zama Ramirez, Pierluigi and Poggi, Matteo and Tosi, Fabio and Mattoccia, Stefano and Di Stefano, Luigi},
    booktitle = {The IEEE International Conference on Computer Vision},
    note = {ICCV},
    year = {2023},
}
</pre>
</section>
