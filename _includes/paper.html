<section class="hero  has-text-centered" id="paper">
    <div class="hero-body">
        <div class="container">
          <h2 align="centered" class="title">Paper CFM4IAD</h2>
            <br>
              <div style="display: flex; gap: 2px; justify-content: center; align-items: center;">
                <h2>[Paper]</h2>
                <h2>[Supplementary]</h2>
                <h2>[Code]</h2>
                <a href="https://github.com/CVLAB-Unibo/crossmodal-feature-mapping"><h2>[Code]</h2></a>
                <!-- <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Costanzino_Learning_Depth_Estimation_for_Transparent_and_Mirror_Surfaces_ICCV_2023_paper.pdf"><h2>[Paper]</h2>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Costanzino_Learning_Depth_Estimation_ICCV_2023_supplemental.pdf"><h2>[Supplementary]</h2>
                <a href="https://github.com/CVLAB-Unibo/Depth4ToM-code#-learning-depth-estimation-for-transparent-and-mirror-surfaces-iccv-2023-"><h2>[Code]</h2></a>
                <a href="https://github.com/CVLAB-Unibo/Depth4ToM-code#file_cabinet-dataset"><h2>[Dataset]</h2></a>
                <a href="https://github.com/CVLAB-Unibo/Depth4ToM/blob/4e6cc00409a0ce4674156f6f4844f077ebd7a5fb/assets/ICCV_2023_poster.pdf"><h2>[Poster]</h2></a>
                <a href="https://www.youtube.com/watch?v=qxJxcsmC0pc"><h2>[Video]</h2></a> -->
                
              </div>

            <div style="display: flex; gap: 10px; justify-content: center; align-items: center;">
              <h2>[Poster]</h2>
              <h2>[Video]</h2>
              <!-- <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Costanzino_Learning_Depth_Estimation_for_Transparent_and_Mirror_Surfaces_ICCV_2023_paper.pdf"><h2>[Paper]</h2>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Costanzino_Learning_Depth_Estimation_ICCV_2023_supplemental.pdf"><h2>[Supplementary]</h2>
              <a href="https://github.com/CVLAB-Unibo/Depth4ToM-code#-learning-depth-estimation-for-transparent-and-mirror-surfaces-iccv-2023-"><h2>[Code]</h2></a>
              <a href="https://github.com/CVLAB-Unibo/Depth4ToM-code#file_cabinet-dataset"><h2>[Dataset]</h2></a>
              <a href="https://github.com/CVLAB-Unibo/Depth4ToM/blob/4e6cc00409a0ce4674156f6f4844f077ebd7a5fb/assets/ICCV_2023_poster.pdf"><h2>[Poster]</h2></a>
              <a href="https://www.youtube.com/watch?v=qxJxcsmC0pc"><h2>[Video]</h2></a> -->
            </div>              
          <br>
                <div class="columns">
                <div class="column is-one-fifth-desktop is-one-fifth-tablet is-one-fifth-fullhd">
                  <br>
                  <br>
                  <br>
                  <a href="https://arxiv.org/abs/2312.04521">
                  <img style="width:90%" src="assets/paper_icon.png" >
                  </a>
                </div>
                <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">

                    <br>
                    <div class="container columns is-centered">
                        <div>
                          <B><a href="https://arxiv.org/abs/2312.04521"><font size = "+2">Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping</font></a><br></B>
                          <B><i> Alex Costanzino*, Pierluigi Zama Ramirez*, Giuseppe Lisanti, Luigi Di Stefano</B></i>
                          <br>
                          <i><font size = "-1">*Equal Contribution</font></i>
                          <br>
                          <br>
                          <p>
                            Recent advancements have shown the potential of leveraging both point clouds and images to localize anomalies.
                            Nevertheless, their applicability in industrial manufacturing is often constrained by significant drawbacks, such as the use of memory banks, which leads to a substantial increase in terms of memory footprint and inference times.
                            We propose a novel light and fast framework that learns to map features from one modality to the other on nominal samples and detect anomalies by pinpointing inconsistencies between observed and mapped features. 
                            Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. 
                            Furthermore, we propose a layer pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.
                          </div>
                        <div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                CITATION
                </h3>
                <div class="has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen form-group col-md-18 col-md-offset-0">
<pre>
@inproceedings{costanzino2024cross,
    title = {Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping},
    author = {Costanzino, Alex and Zama Ramirez, Pierluigi and Lisanti, Giuseppe and Di Stefano, Luigi},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    note = {CVPR},
    year = {2024},
}
</pre>

<br>

<center>
  We gratefully acknowledge the support of SACMI Imola.
</center>

</section>
